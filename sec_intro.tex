\section{Introduction}



An effective image 
representation is a crucial component of image retrieval systems
where, given some query image,  images must be retrieved from a large 
database, then ranked according to their degree of similarity with the 
query.
%Robust image representations are a crucial component of a vast number of computer vision applications. 
%Within these, the \emph{image retrieval} application is an important example wherein a query image is provided to the system, which must in turn find all matching images from within a large, unannotated database. 
The matching process is done entirely based on the pixel content of the query and database images, and the search must be robust to large image variations due to camera pose, color differences and scene illumination, amongst others.


The advances in image representation enabled the success of new systems in this challenging task. %An image representation function commonly maps a given input image to a vector in a fixed dimensional space called the image \emph{feature vector}. 
Indeed, many successful feature representations for image retrieval rely on unsupervised models such as $K$-means \cite{Delhumeau2013} or Gaussian mixture models \cite{Perronnin2010} and, before the neural networks \textit{renaissance}, these representations would outperform methods that exploit supervised learning of image features directly~\cite{Bilen2015,Rana}.
Today, with the popularization of convolutional architectures, global image descriptors are often obtained by aggregation and/or pooling the last convolutional layers \cite{babenko15,GoAlReLa16,KaMeOs16} or by addition of new differentiable layers to an existing architecture~\cite{Arandjelovic15}.
%One of the main reasons for this is the lack of adequately large and varied supervised datasets that are expensive to collect.
%A now pervasive example of an image feature representation is that consisting of the activation coefficients extracted from the previous-to-last layer of Convolutional Neural Networks (CNNs) \cite{Krizhevsky2012}. Although CNNs are trained in a fully-supervised manner for the image classification task, they have been shown to transfer well not only to new, unseen classes \cite{Oquaba,Chatfield2014}, but also to alternate tasks such as object detection \cite{Girshick2014} and, importantly, image retrieval \cite{Sharif}.

The exemplar support vector machine (ESVM), originally proposed by Malisiewicz {\it et al.} \cite{Malisiewicza}, leverages the availability of large, unannotated pools of images within the context of supervised learning. It uses a large generic pool of images as a set of negative examples, while using a single image (the \emph{exemplar}) as a positive example. Given these training sets, an SVM classifier is learned that can  generalize well, despite the drastically limited size of the set of positive examples.

Zepeda \emph{et al.} \cite{ZePe15} propose to treat instead the weights of the resulting classifier as a new feature vector for image retrieval. %This new ESVM feature is derived from \emph{base} feature representations ({\it e.g.,} CNN activation features) of the exemplar and the negative pool.
%An extension \cite{ZePe15} of the ESVM formulation discussed above instead treats the resulting  classifier as an \emph{enhanced} representation of the image feature representing the exemplar image (the \emph{base} feature representation).
An ESVM feature is extracted from each database image, as well as from the query image, by treating each image as an exemplar while keeping a fixed pool of generic negative images. Searching amounts to computing distances between the query and the database ESVM features. Note that ESVM features can be derived from arbitrary \emph{base} features ({\it e.g.}, CNN activations) of the exemplar and the images in the generic negative pool. %An interesting interpretation of this approach is that the ESVM features represent what is unique about the exemplar relative to the pool of generic negatives, which is the role of a classifier.


One important drawback of the ESVM feature encoding approach is that computing the  classifier requires solving an optimization problem for each positive example (\ie each query and each database image). This can be time consuming for the large negative pool sizes required for good ESVM feature performance. In this work we propose using the square loss instead of the hinge loss, in effect converting the ESVM problem into a ridge regression that can be solved in closed form. The parameters of the corresponding classifier, which we call the square loss exemplar machine (SLEM), had been shown to both generalize LDA \cite{Koba15} and, for the general case of multiple positive samples (LS-SVM)~\cite{lssvm}, form features with comparable binary classification performance while drastically reducing the computational cost.


Since computing the SLEM features requires inverting a large matrix related to the training set's covariance matrix, we propose an efficient way to compute this inverse that exploits the fact that only a single (positive) example changes in the training set when computing SLEM features for different images. We show experimentally that our representation matches and even improves upon the performance of ESVM features on two standard datasets using a wide range of base features.

We also introduce a kernelized variant of SLEM that enjoys similar computational advantages but improves retrieval performances. Further computational efficiency is obtained using a kernel principal components analysis (KPCA) decomposition of the negative samples.
%low rank incomplete Cholesky decomposition of the negative pool kernel matrix.

The rest of this paper is organized as follows:
%In Section \ref{prior work} we provide an overview of various existing feature representation methods. 
In Section \ref{lsesvm} we first review the original ESVM feature representation method and subsequently introduce the proposed linear SLEM model. We then introduce the kernelized SLEM variant in Section \ref{nonlinear SLEM}. We then present the low-rank approximation of our method that enables efficient implementations in the non-linear case in Section \ref{eff_imp}. We evaluate our proposed method in image retrieval in Section \ref{eval}, and present conclusions in Section \ref{conclusion}.



% Image search
% - Importance.
% - Feature representations. 

% Feature representation - supervised learning
% - Generic framework.
% - Deep CNN features.
% - ImageNet.

% Image retrieval
% - Supervised - Eusipco, NetVLAD.
% - VLAD, Fisher.
% - Cheaper.

% Pool:
% LDA
% SVMs for feature representations - parts learning, Malizsiewics


%\section {Introduction}
% The exemplar SVM (E-SVM) was first introduced by Malisiewicz et al. in \cite{Efros11} as a conceptually simple framework for object detection and image classification where the training set has a small ratio of positive/negative examples. 
% At training time, many SVMs are learned from a large pool of negative against a single positive (so called exemplar). 
% At test time, the scores of the test images for each classifier are fitted by a logistic regression. 
% The final score of an image is then a non-linear combination of scores from multiples exemplar SVMs.

% They have also been used in \cite{Efros12} in image retrieval tasks, using the classifier score to rank matching candidates.
% However this transfer of information from classification to retrieval is severely limited. 
% Indeed, the purpose of a support vector machine is to separate positive and negative samples, i.e., predict discrete labels. 
% The distance between a negative sample and the classification hyperplane has no value as a measure of has \emph{a priori} no value of continuous matching score.

% Zepeda et al. address this problem in \cite{ZePe15} by firstly, performing an E-SVM to each image in a dataset instead of only for the query images and secondly, comparing its classifiers distance to the query's classifier instead of comparing scores. 
% These modifications guarantee we are ranking distances between two points instead of classification scores.
% Therefore \cite{ZePe15} refers to E-SVMs as features encoders: a pipeline that takes an image representation as input and returns an improved image representation. This type of feature enhancing is more akin to methods such as whitening, PCA and LDA.

% This paper introduces the square-loss Exemplar machine (SLEM), which consists of optimizing the same cost function of a regular E-SVM where the hinge loss is replaced by the square loss. 
% The square loss version has the advantage of having a much more efficient optimization. Indeed, the minimization of its cost function is solved by a linear system and can be done for all exemplar simultaneously, whereas the regular E-SVM cost function is generally minimized one exemplar at a time, normally by stochastic gradient descent \cite{bottou10}.
% Also, for the machine learning tasks of binary classification, both regular SVM and least-squares SVM have similar performances \cite{YeXi07}.
% %Also, for the machine learning tasks of binary classification, both regular SVM and least-squares SVM have the same performance if positive and negative samples are separable and the pool of samples is linearly independent \cite{YeXi07}. 
% We also introduce a kernelized version of SLEM, efficiently implemented, that gives better results and superior scalability for large-scale image retrieval problems.

%%% Local Variables:
%%% TeX-master: "main_eccv"
%%% End:

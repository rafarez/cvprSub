\section{Square loss exemplar machine}\label{lsesvm}
In this section, we revisit the exemplar SVM model presented by \cite{Efros11} as a more general exemplar classifier model. Then, we introduce the square loss exemplar machine (SLEM) by solving a simple variant this model and study its properties.
\subsection{Exemplar classifiers} \label{esvm}
We are given base features in $\RR^d$ at training time, one positive example $x_0$ in $\RR^d$ and a set of negative examples $X = [x_1, x_2,...,x_n]$ in $\RR^{d\times n}$, each column of $X$ representing one example by a vector in $\RR^d$. 
We are also given a loss function $l:\{-1, 1\}\times \RR\rightarrow\RR^+$. Learning an exemplar classifier from these examples amounts to minimizing the function 
\begin{equation}
J(\omega, \nu) = \theta \ l(1, \omega^Tx_0+\nu) +\dfrac{1}{n}\sumi l(-1, \omega^T x_i+\nu)+\dfrac{\lambda}{2}\|\omega\|^2, 
\label{eq:first}
\end{equation}
w.r.t. $\omega$ in $\RR^d$ and $\nu$ in $\RR$. In Equation (\ref{eq:first}), $\lambda$ and $\theta$ are respectively a regularization parameter on $\omega$ and a positive scalar adjusting the weight of the positive exemplar.
%\footnote{We could also acknowledge a parameter $\theta$ other than $\frac{1}{n}$ as a regularization to the error of each negative example. But this parameters seems to be less important to cross-validate. Also, setting $\theta=\frac{1}{n}$ simplify Equation (\ref{omega:solution}) and allows the use of Woodbury identity.} 

The exemplar classifiers of $x_0$ with respect to $X$ are the weights $\omega^\star(x_0,X)$ that minimizes the loss function $J$:
\begin{equation}
\big(\omega^\star(x_0, X), \nu^\star(x_0, X)\big) = \underset{(\omega,\nu)\in\RR^d\times\RR}{\argmin} \ J(\omega, \nu). \label{omega:first}
\end{equation}
To shorten the notation, we refer to these solutions as $\omega^\star$ and $\nu^\star$\footnote{Depending on the loss function $l$, $\nu^\star(x_0,X)$ may be not unique.} when their arguments are implicit.

%\cite{Efros11} introduced ensembles of exemplar classifiers for object detection, but single exemplar classifiers have been used as replacement of the original feature $x_0$ for image classification and retrieval \cite{ZePe15} and detection \cite{GMPD12}. \RAF{Maybe this should be in the introduction instead.}
Applications of ESVM use the hinge loss function, which guarantees that $J$ is a convex function. The solution of Equation (\ref{omega:first}) can be thus found by stochastic gradient descent~\cite{bottou10} individually for each positive sample. The next subsection show how to calculate all exemplar classifiers simultaneously by changing the loss function.

\subsection{Square loss function}\label{slem:intro}
Now, let us study the same learning problem for the square-loss function $l(y,\hat{y}) = \frac{1}{2}(y-\hat{y})^2$. As in the case of the hinge loss, Equation (\ref{eq:first}) is a convex problem. 
However it is now a ridge regression problem, whose unique solution can be found in closed form as
\begin{align}
\begin{cases}
\vspace{3 mm}
\omega^\star &= \dfrac{2\theta}{\theta+1}U^{-1}(x_0-\mu), \\
%\vspace{3 mm}
\nu^\star &= \dfrac{\theta-1}{\theta+1}-\dfrac{1}{\theta+1}(\theta x_0+\mu)^T\omega^\star,
\end{cases}
\label{omega:solution}
\end{align}
%\begin{align}
%\nu^\star &= \dfrac{\theta-1}{\theta+1}-\dfrac{1}{\theta+1}(\theta x_0+\mu)^T\omega^\star, \label{eq:nustar}\\
%\omega^\star &= \dfrac{2\theta}{\theta+1}U^{-1}(x_0-\mu) \label{eq:wstar}
%\end{align}
where:
\begin{align}
\begin{cases}
\mu = &\frac{1}{n}\sum_{i=1}^n x_i,\\ \\
U = &\frac{1}{n}XX^T-\mu\mu^T\\
&+\frac{\theta}{\theta+1}(x_0-\mu)(x_0-\mu)^T+\lambda\mathrm{Id}_d. \label{eq:U}
\end{cases}
\end{align}
Equation (\ref{omega:solution}) shows how to solve (\ref{eq:first}) simultaneously for all exemplars.
%Replacing the hinge loss by the square loss offers a more compact solution, but not necessarily more efficient.

\textbf{Woodbury identity.} 
We can modify Equation \ref{omega:solution} by simplifing Equation \ref{eq:U}.
Let us define $A = \frac{1}{n}XX^T-\mu\mu^T +\lambda\mathrm{Id}_d$ our regularized covariance matrix and assume its inverse $A^{-1}$ known. 
%$A$ is the covariance matrix of the negative samples $X$\footnote{added of a small term proportional to the identity matrix to insure $A$ is positive-definite.}. Let us assume its inverse $A^{-1}$ is known. 
The matrix $U$ now reads $U = A + \frac{\theta}{\theta+1}\delta\delta^T$, where $\delta=x_0-\mu$ is the centralized (w.r.t. the negative mean) positive sample. The Woodbury identity \cite{woodbury} gives us
\begin{equation}
U^{-1} = A^{-1} -\dfrac{\theta}{\theta\delta^TA^{-1}\delta+ \theta+1}A^{-1}\delta^T\delta A^{-1}. \label{invU}
\end{equation}
Substituting (\ref{invU}) in (\ref{omega:solution}) yields
\begin{equation}
\begin{split}
\omega^\star &= \dfrac{2\theta}{\theta +1}\left(A^{-1}\delta - \dfrac{\theta}{\theta\delta^TA^{-1}\delta+ \theta+1} A^{-1}\delta (\delta^TA^{-1}\delta)\right)\\
&= \dfrac{2\theta}{\theta\delta^TA^{-1}\delta+ \theta+1} A^{-1}\delta.\label{Wood:omega}
\end{split}
\end{equation}

Note that the positive sample weight $\theta$ does not influence the direction of the optimal vector $\omega^\star$, only its norm. This means that if search and ranking are based on the normalized feature $\frac{1}{\|\omega^\star\|}\omega^\star$, $\theta$ does not influence the matching score of the SLEM vectors of two different images. This sets SLEM appart from ESVM that requires this parameter to be calibrated \cite{Efros11,ZePe15}. The normalized exemplar classifier $\omega^\star$ is therefore a linear transformation of $x_0-\mu$
%Indeed, if $\omega$ and $\omega'$ are the $d$-dimensional SLEM vectors of $x$ and $x'$, respectively, their cosine similarity reads:
%we can denote $s$ the matching score scalar function defined in $\RR^d\times \RR^d$, which is given by
%\begin{equation}
%s(\omega, \omega') = \dfrac{\omega^T \omega'}{\|\omega\| \|\omega'\|} = \frac{(x-\mu)^TA^{-2}(x'-\mu)}{\|A^{-1}(x-\mu)\|\|A^{-1}(x'-\mu)\|},\label{match:score}
%\end{equation}
%which does not depend on the value of $\theta$. 
%This means that the weight of the positive sample can be fixed at any positive value without changing matching scores. 

\subsection{LDA and SLEM}\label{sec:lda}

Let us now reanalyse Equation \ref{eq:first} for a particular case. Suppose that we have multiple positive samples. It can be shown that in this case, the corresponding linear classifier of Equation (\ref{eq:first}) for the square-loss is also given by
(\ref{omega:solution}), where $x_0$ denotes this time the center of mass
of the positive samples, {\em if} these samples have the {\em same} covariance matrix $\Sigma$ as the negative samples $X$.
 
This equal-covariance assumption is of course quite restrictive, and probably unrealistic in general. It is interesting to note, however, that this is exactly the assumption made by linear discriminant analysis. As shown in~\cite{Hastie2009} for example, LDA can be seen as a (non-regularized) linear classifier with decision function $\omega^T_{LDA} z+ \nu_{LDA}$, where $z$ is a sample in
$\RR^d$, and
\begin{equation}
\left\{\begin{array}{l}
\displaystyle \omega_{LDA}=\Sigma^{-1}(x_0-\mu),\\
\displaystyle \nu_{LDA}=-\frac{1}{2}(x_0+\mu)^T \omega_{LDA},
\end{array}\right.
\label{eq:lda}
\end{equation}
 
Note that SLEM is a generalized version of LDA. Indeed, when $\lambda=0$ in SLEM (i.e. no regularization), $A = \Sigma$, meaning $\omega_{LDA}$ of Eq. (\ref{eq:lda}) and $\omega^\star$ of Eq. (\ref{Wood:omega}) have the same direction.
%This observation has been equally made by \cite{Koba15}.

Many interesting properties of LDA has been used recently for classification tasks \cite{GMPD12,HMR12} and, more recently, for image retrieval with convolutional neural network activations \cite{babenko15}.
By simply generalizing LDA, we hope to obtain superior results.

%In the next section, we take one step further and generalize SLEM with kernel methods. \hlc{something to add to this sentence?}


%%% Local Variables:
%%% TeX-master: "main_eccv"
%%% End:

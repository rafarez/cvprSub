\section{The square-loss exemplar machine}\label{lsesvm}
In this section, we revisit the exemplar SVM model proposed in \cite{Efros11} as an instance of a more general family of classifiers. Then, we introduce the square loss exemplar machine (SLEM) by solving a simple variant of this model and study its properties.
\subsection{Exemplar classifiers} \label{esvm}
We are given base features in $\RR^d$ at training time, one positive example $x_0$ in $\RR^d$ and a set of negative examples $X = [x_1, x_2,...,x_n]$ in $\RR^{d\times n}$, each column of $X$ representing one example by a vector in $\RR^d$. 
We are also given a loss function $l:\{-1, 1\}\times \RR\rightarrow\RR^+$. Learning an exemplar classifier from these examples amounts to minimizing the function 
\begin{equation}
J(\omega, \nu) = \theta \ l(1, \omega^Tx_0+\nu) +\dfrac{1}{n}\sumi l(-1, \omega^T x_i+\nu)+\dfrac{\lambda}{2}\|\omega\|^2, 
\label{eq:first}
\end{equation}
w.r.t. $\omega$ in $\RR^d$ and $\nu$ in $\RR$. In Eq. (\ref{eq:first}), $\lambda$ and $\theta$ are respectively a regularization parameter on $\omega$ and a positive scalar adjusting the weight of the positive exemplar.
%\footnote{We could also acknowledge a parameter $\theta$ other than $\frac{1}{n}$ as a regularization to the error of each negative example. But this parameters seems to be less important to cross-validate. Also, setting $\theta=\frac{1}{n}$ simplify Equation (\ref{omega:solution}) and allows the use of Woodbury identity.} 

Given a cost $l$, we define the correspondent \emph{exemplar classifiers} of $x_0$ with respect to $X$ are the weights $\omega^\star(x_0,X)$ that minimizes the loss function $J$:
\begin{equation}
\big(\omega^\star, \nu^\star\big) = \underset{(\omega,\nu)\in\RR^d\times\RR}{\argmin} \ J(\omega, \nu). \footnote{Depending on the loss function $l$, $\nu^\star(x_0,X)$ may be not unique.}
%\big(\omega^\star(x_0, X), \nu^\star(x_0, X)\big) = \underset{(\omega,\nu)\in\RR^d\times\RR}{\argmin} \ J(\omega, \nu). 
\label{omega:first}
\end{equation}
%To shorten the notation, we refer to these solutions as $\omega^\star$ and $\nu^\star$ when their arguments are implicit.

%\cite{Efros11} introduced ensembles of exemplar classifiers for object detection, but single exemplar classifiers have been used as replacement of the original feature $x_0$ for image classification and retrieval \cite{ZePe15} and detection \cite{GMPD12}. \RAF{Maybe this should be in the introduction instead.}
The exemplar SVM~\cite{Efros11,Efros12} is an instance of this model where $l$ is the hinge loss. 
%Applications of ESVM use the hinge loss function, which guarantees that $J$ is a convex function. 
The solution of Eq. (\ref{omega:first}) can thus be found by stochastic gradient descent~\cite{bottou10} individually for each positive sample. The next section shows how to calculate all exemplar classifiers simultaneously by changing the loss function.

\subsection{The square-loss}\label{slem:intro}
Now, let us study the same learning problem for the square-loss function $l(y,\hat{y}) = \frac{1}{2}(y-\hat{y})^2$. As in the case of the hinge loss, the minimization of Eq. (\ref{eq:first}) is a convex problem. 
However it is now a ridge regression problem, whose unique solution can be found in closed form as
\begin{align}
\begin{cases}
\vspace{3 mm}
\omega^\star &= \dfrac{2\theta}{\theta+1}U^{-1}(x_0-\mu), \\
%\vspace{3 mm}
\nu^\star &= \dfrac{\theta-1}{\theta+1}-\dfrac{1}{\theta+1}(\theta x_0+\mu)^T\omega^\star,
\end{cases}
\label{omega:solution}
\end{align}
%\begin{align}
%\nu^\star &= \dfrac{\theta-1}{\theta+1}-\dfrac{1}{\theta+1}(\theta x_0+\mu)^T\omega^\star, \label{eq:nustar}\\
%\omega^\star &= \dfrac{2\theta}{\theta+1}U^{-1}(x_0-\mu) \label{eq:wstar}
%\end{align}
where:
\begin{align}
\begin{cases}
\mu = &\frac{1}{n}\sum_{i=1}^n x_i,\\ \\
U = &\frac{1}{n}XX^T-\mu\mu^T\\
&+\frac{\theta}{\theta+1}(x_0-\mu)(x_0-\mu)^T+\lambda\mathrm{Id}_d. \label{eq:U}
\end{cases}
\end{align}
%Replacing the hinge loss by the square loss offers a more compact solution, but not necessarily more efficient.

\textbf{Woodbury identity.} 
We can simplify Eq. (\ref{omega:solution}) by modifying $U$ in Eq. (\ref{eq:U}).
Let us define $A = \frac{1}{n}XX^T-\mu\mu^T +\lambda\mathrm{Id}_d$ our regularized covariance matrix and assume its inverse $A^{-1}$ known. 
%$A$ is the covariance matrix of the negative samples $X$\footnote{added of a small term proportional to the identity matrix to insure $A$ is positive-definite.}. Let us assume its inverse $A^{-1}$ is known. 
The matrix $U$ now reads $U = A + \frac{\theta}{\theta+1}\delta\delta^T$, where $\delta=x_0-\mu$ is the centralized (w.r.t. the negatives' mean) positive sample. The Woodbury identity \cite{woodbury} gives us
\begin{equation}
U^{-1} = A^{-1} -\dfrac{\theta}{\theta\delta^TA^{-1}\delta+ \theta+1}A^{-1}\delta^T\delta A^{-1}. \label{invU}
\end{equation}
Substituting (\ref{invU}) in (\ref{omega:solution}) yields
\begin{equation}
\begin{split}
\omega^\star &= \dfrac{2\theta}{\theta +1}\left(A^{-1}\delta - \dfrac{\theta}{\theta\delta^TA^{-1}\delta+ \theta+1} A^{-1}\delta (\delta^TA^{-1}\delta)\right)\\
&= \dfrac{2\theta}{\theta\delta^TA^{-1}\delta+ \theta+1} A^{-1}\delta.\label{Wood:omega}
\end{split}
\end{equation}

Note that the positive sample weight $\theta$ does not influence the direction of the optimal vector $\omega^\star$, only its norm. This means that if search and ranking are based on the normalized feature $\frac{1}{\|\omega^\star\|}\omega^\star$, \eg using cosine similarity, $\theta$ does not influence the matching score of the SLEM vectors of two different images. This sets SLEM appart from ESVM which requires this parameter to be calibrated \cite{Efros11,ZePe15}. We can thus set $\theta=\frac{1}{n}$ for the remaining of this work.

%Equation (\ref{Wood:omega}) also shows how to solve (\ref{eq:first}) in closed form and, hence, simultaneously for all individual exemplars by solving a linear system~\cite{lssvm}. If $X_p$ is the matrix where every column represents one positive sample, we obtain their respective exemplar classifier by solving the system $A\Omega = \bar{X}_p$ in $\Omega$. Here, $\bar{X}_p$ denotes the $X_p$ centralized w.r.t. the negatives' mean. It 
%The normalized exemplar classifier $\omega^\star$ is therefore a linear transformation of $x_0-\mu$.
%Indeed, if $\omega$ and $\omega'$ are the $d$-dimensional SLEM vectors of $x$ and $x'$, respectively, their cosine similarity reads:
%we can denote $s$ the matching score scalar function defined in $\RR^d\times \RR^d$, which is given by
%\begin{equation}
%s(\omega, \omega') = \dfrac{\omega^T \omega'}{\|\omega\| \|\omega'\|} = \frac{(x-\mu)^TA^{-2}(x'-\mu)}{\|A^{-1}(x-\mu)\|\|A^{-1}(x'-\mu)\|},\label{match:score}
%\end{equation}
%which does not depend on the value of $\theta$. 
%This means that the weight of the positive sample can be fixed at any positive value without changing matching scores. 

\subsection{LDA and SLEM}\label{sec:lda}

It is interesting to note the relationship between SLEM and the classical linear discriminant analysis (LDA).
Let us return to Eq. (\ref{eq:first}) an suppose that we have multiple positive samples. It can be shown that in this case, the corresponding linear classifier of Eq. (\ref{eq:first}) for the square-loss is also given by
(\ref{omega:solution}), where $x_0$ denotes this time the center of mass
of the positive samples {\em if} these samples have the {\em same} covariance matrix $\Sigma$ as the negative samples $X$.
 
This equal-covariance assumption is of course quite restrictive, and probably unrealistic in general. It is interesting to note, however, that this is exactly the assumption made by linear discriminant analysis. As shown in~\cite{Hastie2009} for example, LDA can be seen as a (non-regularized) linear classifier with decision function $\omega^T_{LDA} z+ \nu_{LDA}$, where $z$ is a sample in
$\RR^d$, and
\begin{equation}
\left\{\begin{array}{l}
\displaystyle \omega_{LDA}=\Sigma^{-1}(x_0-\mu),\\
\displaystyle \nu_{LDA}=-\frac{1}{2}(x_0+\mu)^T \omega_{LDA}.
\end{array}\right
\label{eq:lda}
\end{equation}
 
This shows that SLEM is a generalized version of LDA: Indeed, taking $\lambda=0$ (\ie no regularization), $A = \Sigma$ and the vectors $\omega_{LDA}$ of Eq. (\ref{eq:lda}) and $\omega^\star$ of Eq. (\ref{Wood:omega}) have the same direction, reducing SLEM to LDA.
%This observation has been equally made by \cite{Koba15}.
Many interesting properties of LDA have been used recently for classification tasks \cite{GMPD12,HMR12} and, more recently, for image retrieval with convolutional neural network activations \cite{babenko15}.
With our simple generalization of LDA, we hope to obtain superior results.

%In the next section, we take one step further and generalize SLEM with kernel methods. \hlc{something to add to this sentence?}


%%% Local Variables:
%%% TeX-master: "main_eccv"
%%% End:

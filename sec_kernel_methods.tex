\section{The kernel SLEM}
\label{nonlinear SLEM}

\subsection{Kernel methods}\label{kernel:review}
Let us recall a few basic facts about kernel methods for supervised
classification. We consider a reproducing kernel Hilbert space (RKHS)
$H$ formed by real functions over some set
$X$, and denote by $k$ and $\varphi$ the corresponding reproducing kernel and feature map (which may not admit a known explicit form) over $X$, respectively. We
address the following learning problem over $H\times\RR$:
%\begin{equation}
%\min_{h\in H,\nu\in\RR}
%\sum_{i=1}^n l(y_i,h(x_i)+\nu) + \frac{\lambda}{2}\|h\|_H^2,
%\label{eq:kernel}
%\end{equation}  
%By definition of a reproducing kernel,
%Equation (\ref{eq:kernel}) can be rewritten as
\begin{equation}
\min_{h\in H,\nu\in\RR}
\dfrac{1}{n}\sum_{i=1}^n l(y_i,\langle \varphi(x_i),h\rangle+\nu) +
\dfrac{\lambda}{2}\|h\|_H^2,
\label{ker:aff}
\end{equation} 
where the pairs $(x_i,y_i)$ in $X\times \{-1,1\}$, $i=1\dots n$ are training samples, %and $l: \{-1,1\}\times \RR\rightarrow\RR^+$ is some arbitrary loss function. 
%$\varphi$ is the {\em feature map} over $X$ associated with the kernel $k$ (which may not admit a known explicit form) 
and $\langle h, h' \rangle$ is the inner product of element $h$ and $h'$ in $H$. We dub problems with the general form of (\ref{ker:aff}) {\em affine}
supervised learning problems since, given some fixed element $h$ of
$H$ and some scalar $\nu$, $\langle h,h'\rangle+\nu$ is an affine function of $h'$,
whose zero set defines an affine hyperplane of $H$ considered itself
as an affine space.

Let $K$ denote the kernel matrix with entries $k_{ij}=\langle\varphi(x_i),
\varphi(x_j)\rangle$ and rows $k_i^T=[k_{i1}, k_{i2},...,k_{in}]$, $i$ in $\{1,\ldots,n\}$.  We assume from now on that $l$ is convex and continuous. Under this assumption, Eq. (\ref{ker:aff}) admits an equivalent formulation
\begin{align}
\min_{\alpha\in\RR^{n}, \ \nu\in\RR} \left(\dfrac{1}{n}\sumi l(y_i, k_i^T\alpha+\nu)  +\dfrac{\lambda}{2}\alpha^TK\alpha\right), \label{ker:first}
\end{align}
and any solution $(\alpha^\star,\nu^\star)$ to (\ref{ker:first})
provides
a solution $(h^\star,\nu^\star)$ to (\ref{ker:aff}) with
$h^\star=\sum_{i=1}^n \alpha_i^\star\varphi(x_i)+\nu^\star$. This result follows from the Riesz representation theorem~\cite{SHS01,Wahba90}.


%where $K$ is a $(n+1)\times (n+1)$  matrix and $k_i$ is its $(i+1)$-th column matrix for $i$ in $\{0, 1, ..., n\}$.\\



Assuming our reproducing kernel is semidefinite positive, 
%\footnote{\textit{i.e.} for any $z$ in $\RR^n$, $z^TKz\ge 0$. This is equivalent to all eigenvalues of $K$ being non-negative.} 
$K$ is a semidefinite positive matrix and can be decomposed as $K=BB^T$.
%, with $B$ a rank $r$ upper triangular matrix of size $n\times r$. 
Using this factorization, the kernelized problem can be expressed as 
\begin{equation}
\min_{\beta\in\RR^r,\nu\in\RR} \left( \dfrac{1}{n}\sumi l(y_i, b_i^T\beta+\nu)+\dfrac{\lambda}{2}\|\beta\|^2\right), \label{beta:first}
\end{equation}
where $b_i^T$ denotes the $i$-th row of $B$ and $r$ is the number of columns of $B$. 
If $(\beta^\star, \nu^\star)$ is the solution Equation (\ref{beta:first}), the corresponding vector $\alpha^\star$ (or, more correctly, \textit{a} corresponding vector of dimension $n\geq r$) 
can be computed by $\alpha^\star=P\beta^\star$, where $P$ is the pseudoinverse of $B^T$.

Note that Eq. (\ref{beta:first}) allows us to write the kernel learning problem (\ref{ker:aff}) as an instance of Eq. (\ref{eq:first}) by setting $y_i=-1$ for all but one training sample. For our approach, we wish to solve (\ref{beta:first}) for many positive training samples (one at a time) against the same set of negative training samples. In the following subsections, we show how to take advantage of the fixed negative samples to efficiently solve (\ref{beta:first}).

\subsection{Offline preprocess of negative samples}\label{offline}
In order to calculate offline all operations that are dependent only on negative samples, let us suppose that the positive training sample is unknown and $K$ is the kernel matrix of the negative samples $X$.
The preprocessing phase consists of the calculation of the decomposition $B$ and the constants of Eq. (\ref{Wood:omega}): $\mu = \frac{1}{n}\sum_{i=1}^n b_i^T$ and $A = \frac{1}{n}B^TB-\mu\mu^T +\lambda\mathrm{Id}_r.$
These operations are done offline and their results are stored. In the processing of positive samples, \emph{i.e.} the online phase, these results are loaded.
%Equation (\ref{beta:first}) has the same structure of Eq. (\ref{eq:first}) if we were to consider $\theta=0$ and all labels $y_i=-1$.

\subsection{Online addition of a positive sample}
\label{subsec:adding}
We now wish to write Eq. (\ref{beta:first}) as an exemplar classifier, with one positive example $x_0$ and $n$ negative examples $X$.
%Let us assume the factor $B$ is given. %The calculation of a kernel Cholesky factorization is solved efficiently by \cite{BaJo05,BaJo02}. 
%We further analyse how to compute this factorization in subsection \ref{offline}.
We denote by $K'$ the augmented kernel matrix obtained by adding the positive samples $x_0$. Such a matrix can be written as
\begin{equation}
K' = \begin{bmatrix}
k_{00} & k_0^T\\
k_0 & K
\end{bmatrix},
\end{equation}
where $k_{00}=\langle \varphi(x_0),\varphi(x_0)\rangle$ is a scalar and $k_0= [\langle \varphi(x_0),\varphi(x_i)\rangle]_{1\le i\le n}$ is a vector in $\RR^n$. 
The following lemma show how the factorization of $K'$ can be derived from the factorization of its sub-matrix $K$ and the solution of a $n\times n$ linear system.

\begin{lemma} The augmented kernel matrix $K'$ can be factorized as $K'= B'B'^T$ with
%\begin{align}
%K'&= B'B'^T,\quad\text{where}\quad
%B'=\begin{bmatrix}
%u & v^T\\0 & B
%\end{bmatrix}\\
%v&=B^\dagger k_0,\,\, u=\sqrt{k_{00}-||v||^2}.\label{eq:lemma1}
%\end{align}
\begin{equation}
B'=\begin{bmatrix}
u & v^T\\0 & B
\end{bmatrix},~
v = B^\dagger k_0,~ u=\sqrt{k_{00}-||v||^2}.
\label{eq:lemma1}
\end{equation}
\end{lemma}\label{lemma1}
\begin{proof}
For $B'$ defined by (\ref{eq:lemma1}), we have that
\begin{equation}
B'B'^T = 
\begin{bmatrix} u^2+\|v\|^2 & v^TB^T\\ 
Bv& BB^T\end{bmatrix} 
=\begin{bmatrix} k_{00} & v^TB^T\\ 
Bv& K\end{bmatrix} .
\end{equation}
Since $K'$ is positive semidefinite, $k_0$ must lie in the column space $\mathcal{B}$ of $B$.
%\footnote{
Indeed, if we suppose $k_0$ does not belong to $\mathcal{B}$, then it can be decomposed uniquely as $k_0=s+t$, $s\in\mathcal{B}$ and  $t\in\mathcal{B}^\perp$, with $t\ne 0$. In one hand, $K'$ being semidefinite positive implies that $[1, -at^T]K'[1; -at]=k_{00}-2a\|t\|^2\ge 0$ for all real value $a$. In the other hand, for $a$ large enough, $k_{00}-a\|t\|^2\le 0$, which is a contradiction.
%} 
Hence $v=B^\dagger k_0$ is an exact solution of $Bv=k_0$. The fact that $k_{00}-\|v\|^2$ is non-negative comes from the fact that the Schur complement $K-k_0 k_0^T / k_{00}$ of $k_{00}$ in $K'$ is itself positive semidefinite.
%\footnote{
Indeed, since the matrix $k_{00}K-k_0k_0^T=B(k_{00}\mathrm{Id}_r-vv^T$ is also positive semidefinite. Thus $v^T(k_{00}\mathrm{Id}_r-vv^T)v = \|v\|^2(k_{00}-\|v\|^2)\ge 0$. \qedhere
%}
\end{proof}


This lemma allows us to add a positive sample to Eq. (\ref{beta:first}).
%$\beta^\star(x_0,X), \nu^\star(x_0,X) = \underset{\beta\in\RR^{r+1}, \nu\in\RR}{argmin}J'(\beta, \nu)$, for
With a positive exemplar, Eq. (\ref{beta:first}) now reads
\begin{align}
 \dfrac{1}{n}l(1, b_0'^T\beta+\nu) +\dfrac{1}{n}\sumi l(-1,b_i'^T\beta+\nu)
+\dfrac{\lambda}{2}\|\beta\|^2,\label{beta:final}
\end{align}
with $b_i'^T$ being the $(i+1)$-th row of $B'$, $i$ in $\{0,1,...,n\}$. In particular, $b_0'=[u; \ v]$ and, for $i>0$, $b_i'=[0; \ b_i]$. The solution $(\beta^\star,\nu^\star)$ in $\RR^{r+1}\times\RR$ can be computed just as before by Equation (\ref{omega:solution}), replacing $x_0$ by $b_0'$, $\mu$ by $\mu' = \frac{1}{n}\sum_{i=1}^n b_i'$ and $X$ by the $(r+1)\times n$ matrix $Q$ of columns $b_1', b_2',...,b_n'$. $\alpha^\star$ is now calculated as
$\alpha^\star=P'\beta^\star$, where $P' = [u^{-1} \ 0^T; -u^{-1}Pv \ P]$ is the pseudoinverse of $B'^T$. 
%This can be expressed by the linear system


\subsection{Similarity score}\label{simi_score}
Once the optimal parameters $(\beta, \nu)$ from (\ref{beta:final}) and the coordinates $u$, $v$ of $b_0'$ from (\ref{eq:lemma1}) have been found\footnote{We drop the ``$\star$'' in this subsection to avoid cluttering the notation.}, they can be used directly for measuring similarity between matching images.

Indeed, suppose two image descriptors $x_0$ and $x_0'$ are given and we wish to calculate the similarity score between theirs SLEM representations $h$ and $h'$, denoted by $s(h,h')$. 
We write $h'=\alpha_0'\varphi(x_0')+\sum_{i=1}^n \alpha_i'\varphi (x_i)+\nu'$. 
Moreover, $\alpha$ can be expressed by the linear system
\begin{equation}
\begin{bmatrix} \alpha_0 \\ \hat{\alpha} \end{bmatrix} = \begin{bmatrix} \frac{1}{u} & 0^T \\-\frac{1}{u}Pv & P  \end{bmatrix} \begin{bmatrix}\beta_0 \\ \hat{\beta} \end{bmatrix}.\label{alpha:to:beta}
\end{equation}
Using Eq. (\ref{alpha:to:beta}) and ignoring bias $\nu$ and $\nu'$ which have empirically no influence, $s(h,h')$ is given by:
\begin{equation}
\begin{split}
s(h,h') & = \langle h, h'\rangle \\
		& = \hat{\alpha}^{T} K\hat{\alpha}'+\alpha_0k(X, x_0)^T\hat{\alpha}'+\alpha_0'k(X, x_0')^T\hat{\alpha} \\
		& \ \ \ \ \ +\alpha_0\alpha_0'k(x_0,x_0')\\
		& = \hat{\beta}^T\hat{\beta}'+\lambda^{-2}(k(x_0,x_0')-v^T).
		%& = \hat{\beta}^T\hat{\beta}'+\frac{\beta_0\beta_0'}{uu'}(k(x_0,x_0')-v^T).
\end{split}
\end{equation}

For a given image whose descriptor is $x_0$, we need to store $x_0$, $\hat{\beta}$ and $v$ to calculate its matching score to whichever other image for SLEM. Each image depends therefore on a vector of $p+2r$ dimensions.

%%% Local Variables:
%%% TeX-master: "main_eccv"
%%% End:
